{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification dataset shape: (90000, 50)\n",
      "[get_rank_dict] Starting combined baseline evaluation and SHAP calculation...\n",
      "Fold: 1/3 | Status: Done (0.45s)              | Fold Time:   0.45s | Total Time:    0.57s                              \n",
      "Fold: 2/3 | Status: Done (0.28s)              | Fold Time:   0.28s | Total Time:    0.85s                              \n",
      "Fold: 3/3 | Status: Done (0.28s)              | Fold Time:   0.28s | Total Time:    1.13s                              \n",
      "------------------------------\n",
      "[get_rank_dict] >> FINAL Baseline Performance (All Features)\n",
      "    Mean CV Score: 0.829\n",
      "    Fold Scores: [0.828 0.817 0.842]\n",
      "[get_rank_dict] Completed calculation. Total time: 1.22 seconds.\n",
      "Rank dict (classification) top-5: {'moonlight_whisper': 1, 'echoes_from_abyss': 2, 'shadow_of_the_north': 3, 'valkyrie_helm_35_lost': 4, 'pixie_dust_14_lost': 5}\n",
      "[get_vars] Evaluating initial feature set (if any)...\n",
      "[get_vars] Starting feature selection procedure...\n",
      "[get_vars] Starting from scratch (will check top 10 features first).\n",
      "\n",
      "--- Starting Step: Selecting feature #1 (Checking top 10) ---\n",
      "    CV Before Step: Inf | Target Summa Threshold to Add: 1\n",
      "Step 1 | Checks since last best: 08/15 | Checking cand. [10/10]: dragon_scale_28_lost    | Best valid cand. (step): echoes_from_abyss       (Summa: 3, CV: 0.752)\n",
      "--- Step finished in 9.09 seconds ---\n",
      "[+] Feature Added: 'echoes_from_abyss'\n",
      "    New Best Mean CV: 0.752 (Achieved Summa: 3)\n",
      "    Selected Features (1): ['echoes_from_abyss']\n",
      "    Meta info saved to df_meta_info_example_classification.csv\n",
      "\n",
      "--- Starting Step: Selecting feature #2 (Checking 48) ---\n",
      "    CV Before Step: 0.752 | Target Summa Threshold to Add: 1\n",
      "Step 2 | Checks since last best: 15/15 | Checking cand. [17/48]: phoenix_feather_29_lost | Best valid cand. (step): shadow_of_the_north     (Summa: 3, CV: 0.800)\n",
      "[get_vars] Reached step attempt limit (15 checks without improvement). Stopping search for this step.\n",
      "\n",
      "--- Step finished in 15.28 seconds ---\n",
      "[+] Feature Added: 'shadow_of_the_north'\n",
      "    New Best Mean CV: 0.800 (Achieved Summa: 3)\n",
      "    Selected Features (2): ['echoes_from_abyss', 'shadow_of_the_north']\n",
      "    Meta info saved to df_meta_info_example_classification.csv\n",
      "\n",
      "--- Starting Step: Selecting feature #3 (Checking 47) ---\n",
      "    CV Before Step: 0.800 | Target Summa Threshold to Add: 1\n",
      "Step 3 | Checks since last best: 15/15 | Checking cand. [16/47]: phoenix_feather_29_lost | Best valid cand. (step): moonlight_whisper       (Summa: 3, CV: 0.831)\n",
      "[get_vars] Reached step attempt limit (15 checks without improvement). Stopping search for this step.\n",
      "\n",
      "--- Step finished in 12.63 seconds ---\n",
      "[+] Feature Added: 'moonlight_whisper'\n",
      "    New Best Mean CV: 0.831 (Achieved Summa: 3)\n",
      "    Selected Features (3): ['echoes_from_abyss', 'shadow_of_the_north', 'moonlight_whisper']\n",
      "    Meta info saved to df_meta_info_example_classification.csv\n",
      "\n",
      "--- Starting Step: Selecting feature #4 (Checking 46) ---\n",
      "    CV Before Step: 0.831 | Target Summa Threshold to Add: 1\n",
      "Step 4 | Checks since last best: 15/15 | Checking cand. [36/46]: forgotten_artifact_41   | Best valid cand. (step): phoenix_feather_5_lost  (Summa: 3, CV: 0.834)\n",
      "[get_vars] Reached step attempt limit (15 checks without improvement). Stopping search for this step.\n",
      "\n",
      "--- Step finished in 31.14 seconds ---\n",
      "[+] Feature Added: 'phoenix_feather_5_lost'\n",
      "    New Best Mean CV: 0.834 (Achieved Summa: 3)\n",
      "    Selected Features (4): ['echoes_from_abyss', 'shadow_of_the_north', 'moonlight_whisper', 'phoenix_feather_5_lost']\n",
      "    Meta info saved to df_meta_info_example_classification.csv\n",
      "\n",
      "--- Starting Step: Selecting feature #5 (Checking 45) ---\n",
      "    CV Before Step: 0.834 | Target Summa Threshold to Add: 1\n",
      "Step 5 | Checks since last best: 15/15 | Checking cand. [25/45]: phoenix_feather_17_lost | Best valid cand. (step): None                    (Summa: N/A, CV: N/A)\n",
      "[get_vars] Reached step attempt limit (15 checks without improvement). Stopping search for this step.\n",
      "\n",
      "--- Step finished in 19.19 seconds ---\n",
      "[-] No feature found that improves the score in this step. Stopping.\n",
      "------------------------------\n",
      "[get_vars] Final feature set:\n",
      "  Features (4): ['echoes_from_abyss', 'shadow_of_the_north', 'moonlight_whisper', 'phoenix_feather_5_lost']\n",
      "  Best Mean CV achieved: 0.834\n",
      "  Baseline Mean CV (All Features): 0.829\n",
      "[get_vars] Completed in 87.81 seconds.\n",
      "Selected vars (classification): ['echoes_from_abyss', 'shadow_of_the_north', 'moonlight_whisper', 'phoenix_feather_5_lost']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='shap')\n",
    "\n",
    "# seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# import classes\n",
    "from Tools import DateTimeSeriesSplit, Kraken\n",
    "\n",
    "# model and metric for classification\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# example 2: classification\n",
    "c_clf = 30000  # Увеличили количество строк\n",
    "\n",
    "# create dataset parts\n",
    "Xc1 = pd.DataFrame()\n",
    "\n",
    "# Основные значимые фичи\n",
    "Xc1['shadow_of_the_north'] = np.random.rand(c_clf)       # Бывший var_1\n",
    "Xc1['moonlight_whisper'] = np.random.rand(c_clf)         # Бывший var_2  \n",
    "Xc1['echoes_from_abyss'] = np.random.rand(c_clf)         # Бывший var_3\n",
    "\n",
    "# Шумовые фичи с мифическими названиями\n",
    "noise_features = [\n",
    "    'dragon_scale', 'phoenix_feather', 'unicorn_tear',\n",
    "    'griffin_claw', 'mermaid_song', 'basilisk_gaze',\n",
    "    'kraken_tentacle', 'valkyrie_helm', 'sphinx_riddle',\n",
    "    'centaur_hoof', 'pixie_dust', 'werewolf_fang'\n",
    "]\n",
    "\n",
    "for idx in range(4, 50):\n",
    "    feature_name = f\"{noise_features[(idx-4)%12]}_{idx}_lost\" if idx < 40 else f\"forgotten_artifact_{idx}\"\n",
    "    Xc1[feature_name] = np.random.rand(c_clf)\n",
    "\n",
    "# Временная метка\n",
    "Xc1['scroll_of_chronicles'] = pd.date_range(start='2005-01-01', periods=c_clf, freq='D')\n",
    "\n",
    "# Целевая переменная\n",
    "y_c1_float = (4 * Xc1['shadow_of_the_north'] + \n",
    "              5 * Xc1['moonlight_whisper'] + \n",
    "              (2*Xc1['echoes_from_abyss'])**2 + \n",
    "              1.0 * np.random.rand(c_clf))\n",
    "y_c1 = (y_c1_float > 6.0).astype(int)\n",
    "\n",
    "# Создаем копии с небольшими вариациями\n",
    "Xc2 = Xc1.copy()\n",
    "y_c2 = ((2 * Xc2['shadow_of_the_north'] + \n",
    "         2 * Xc2['moonlight_whisper'] + \n",
    "         (2*Xc2['echoes_from_abyss'])**1.9) + \n",
    "        1.0*np.random.rand(c_clf) > 6.0).astype(int)\n",
    "\n",
    "Xc3 = Xc1.copy()\n",
    "y_c3 = ((3 * Xc3['shadow_of_the_north'] + \n",
    "         3 * Xc3['moonlight_whisper'] + \n",
    "         (2*Xc3['echoes_from_abyss'])**1.5) + \n",
    "        1.0*np.random.rand(c_clf) > 6.0).astype(int)\n",
    "\n",
    "Xc = pd.concat([Xc1, Xc2, Xc3], axis=0)\n",
    "y_c = pd.concat([y_c1, y_c2, y_c3], axis=0).reset_index(drop=True)\n",
    "print(\"Classification dataset shape:\", Xc.shape)\n",
    "\n",
    "cv_datetime_clf = DateTimeSeriesSplit(window=1500, n_splits=3, test_size=300, margin=0)\n",
    "group_dt_clf = Xc['scroll_of_chronicles']\n",
    "\n",
    "# Формируем список признаков\n",
    "vars_for_clf = [col for col in Xc.columns if col not in ['scroll_of_chronicles', 'index_time']]\n",
    "model_clf = LGBMClassifier(\n",
    "    max_depth=3, \n",
    "    objective='binary', \n",
    "    verbosity=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Метрика accuracy\n",
    "def my_accuracy(y_true, y_pred_prob):\n",
    "    y_bin = (y_pred_prob > 0.5).astype(int)\n",
    "    return accuracy_score(y_true, y_bin)\n",
    "\n",
    "selector_clf = Kraken(\n",
    "    estimator=model_clf,\n",
    "    cv=cv_datetime_clf,\n",
    "    metric=my_accuracy,\n",
    "    meta_info_name='example_classification',\n",
    "    task_type='classification',\n",
    "    greater_is_better=True,\n",
    "    which_class_for_shap=1,\n",
    "    comparison_precision=3\n",
    ")\n",
    "\n",
    "# Расчет важности признаков\n",
    "selector_clf.get_rank_dict(Xc, y_c, vars_for_clf, group_dt_clf)\n",
    "print(\"Rank dict (classification) top-5:\", dict(list(selector_clf.rank_dict.items())[:5]))\n",
    "\n",
    "# Жадный отбор признаков\n",
    "best_vars_clf = selector_clf.get_vars(\n",
    "    X=Xc, \n",
    "    y=y_c, \n",
    "    rank_dict=selector_clf.rank_dict,\n",
    "    group_dt=group_dt_clf,\n",
    "    max_feature_search_rounds=15,\n",
    "    top_n_for_first_step=10\n",
    ")\n",
    "print(\"Selected vars (classification):\", best_vars_clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
